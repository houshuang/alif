# Backend Services

All services in `backend/app/services/`.

## Scheduling & Review
- `fsrs_service.py` — FSRS spaced repetition. Snapshots pre-review state in fsrs_log_json for undo. Safety-net auto-create for unknown ULK (normally handled by acquisition path in sentence_review_service). Triggers `regenerate_memory_hooks_premium` on fresh lapse (background thread).
- `sentence_selector.py` — Session assembly: greedy set cover, comprehension-aware recency (7d/2d/4h), difficulty matching, easy-bookend ordering. Focus cohort filtering (MAX_COHORT_SIZE=200). **Reserved-slot auto-introduction**: always reserves `INTRO_RESERVE_FRACTION` (20%) of session slots for new words when accuracy allows, even when due queue exceeds limit. Also fires when session is undersized (backward-compat). `_intro_slots_for_accuracy()` maps recent accuracy to graduated rate (<70%→0, 70-85%→4, 85-92%→7, ≥92%→10 slots). Per-call cap: MAX_AUTO_INTRO_PER_SESSION=10. **Fill phase**: when session is still undersized after main assembly + on-demand generation, a second auto-introduce pass runs. Aggressive within-session repetition for acquiring words (MIN_ACQUISITION_EXPOSURES=4, multi-pass expanding intervals, MAX_ACQUISITION_EXTRA_SLOTS=15). **Within-session scaffold diversity**: tracks scaffold words across greedy set cover iterations, applies `SESSION_SCAFFOLD_DECAY` (0.5) exponential penalty per reuse — prevents the same scaffold words from dominating every sentence. Comprehensibility gate (≥60% known scaffold words; acquiring box-1 excluded, encountered excluded — only actively studied words count as known). On-demand sentence generation: multi-target first (groups of 2-4), single-target fallback, parallelized via ThreadPoolExecutor (max 8 workers). Graceful degradation: if on-demand generation fails (DB locked, LLM error), session builder returns existing sentences instead of 500ing. No word-only fallbacks. **Variant→canonical resolution**: sentences with variant forms correctly cover canonical due words; `effective_id` used for scheduling only, `WordMeta.lemma_id` uses original `sw.lemma_id` for display/lookup. **Book sentence preference**: 1.3x source_bonus for `source="book"` sentences over LLM-generated. `compute_sentence_diversity_score()` logs per-sentence metrics (scaffold_uniqueness, scaffold_freshness) for monitoring. **Selection transparency**: each `SentenceReviewItem` includes `selection_info` dict with `reason` (greedy_cover/acquisition_repeat/on_demand/fill_intro), `score`, `order`, `word_reason` (human-readable primary word state), and `components` (per-factor score breakdown: due_coverage, difficulty_match, grammar_fit, diversity, freshness, source_bonus, session_diversity, rescue). **Fast mode performance**: when `skip_on_demand=True`, auto-introduction skips material generation (`skip_material_gen=True`) and lemma backfill uses fast dictionary lookup only (no CAMeL disambiguation) — reduces session build from ~18s to ~1.2s.
- `sentence_review_service.py` — Reviews ALL words equally. Routes acquiring→acquisition, skips encountered. **Collateral credit**: unknown words auto-introduced into acquisition (source="collateral") instead of straight to FSRS. **Variant→canonical redirect**: reviews of variant words credit the canonical lemma, with surface forms tracked in variant_stats_json. credit_type is metadata only. Post-review leech check for words rated ≤2. Undo restores pre-review state from snapshots.
- `acquisition_service.py` — Leitner 3-box (4h→1d→3d). **Two-phase advancement**: box 1→2 always allowed (encoding), box 2→3 and graduation require `acquisition_next_due <= now` (consolidation). Graduation: box≥3 + times_seen≥5 + accuracy≥60% + reviews on ≥2 UTC calendar days. `GRADUATION_MIN_CALENDAR_DAYS=2`. `start_acquisition()` accepts `due_immediately=True` for auto-introduced words. **Preserves existing meaningful source** (book, duolingo, textbook_scan, etc.) — only overwrites generic "study"/"encountered" sources. Triggers `regenerate_memory_hooks_premium` on box demotion from 2+ → 1 (background thread).
- `cohort_service.py` — Focus cohort: MAX_COHORT_SIZE=200. Acquiring words always included, rest filled by lowest-stability due words.
- `leech_service.py` — Auto-manage failing words. Detection: times_seen≥5 AND accuracy<50%. **Graduated cooldown**: 3d (1st) → 7d (2nd) → 14d (3rd+) via `leech_count` on ULK. Stats preserved on reintroduction (not zeroed). Fresh sentences generated and memory hooks ensured on reintro.

## Word Selection & Introduction
- `word_selector.py` — Next-word algorithm with **strict priority tiers**: Tier 1 active book (deterministic page order: `200 - page*2.0`), Tier 2 active stories (+10), Tier 3 OCR/textbook_scan (+8), Tier 4 duolingo (+6), Tier 5 avp_a1 (+4), Tier 6 wiktionary/other (+0, strictly by freq). Within-tier tiebreakers: 40% freq + 30% root + 20% recency + 10% grammar + encountered bonus + category penalty. Topic acts as tiebreaker within OCR/Duolingo only (+0.3). Root-sibling interference guard. Excludes wiktionary refs and variant lemmas. `select_next_words()` returns `story_id` for book/story candidates. introduce_word() calls start_acquisition(), accepts `due_immediately` param for auto-introduction during sessions. **Auto-introduction source attribution**: `_auto_introduce_words` derives ULK source from priority tier (book→"book", active_story→"story_import", textbook_scan/duolingo→as-is, other→"auto_intro") and sets `Lemma.source_story_id` for book/story words. Never picks proper_name, onomatopoeia, or function words.
- `topic_service.py` — Topical learning cycles. 20 domains, MAX_TOPIC_BATCH=15, MIN_TOPIC_WORDS=5. Auto-advance when exhausted/depleted.

## Sentence Generation & Validation
- `sentence_generator.py` — LLM generation with 7-attempt retry loop, diversity weighting, full diacritics. Feeds validation failures back as retry feedback. Post-validation Claude Haiku quality review gate (naturalness + translation accuracy). Accepts `model_override` param (cron uses `claude_sonnet`, on-demand uses `gemini`). Multi-target generation: `generate_validated_sentences_multi_target()` + `group_words_for_multi_target()` for generating sentences covering 2-4 target words simultaneously.
- `sentence_validator.py` — Rule-based: tokenize → strip diacritics → strip clitics → match known forms. `FUNCTION_WORDS` populated from `FUNCTION_WORD_GLOSSES` keys (~80 particles/prepositions/pronouns) — excluded from story/book word counts, book page introduction, FSRS review credit, and scaffold diversity. `FUNCTION_WORD_GLOSSES` also provides fallback glosses. `FUNCTION_WORD_FORMS` kept for clitic analysis prevention. Public API: lookup_lemma(), resolve_existing_lemma(), build_lemma_lookup(), build_comprehensive_lemma_lookup() (ALL lemmas for sentence_word mapping). `validate_sentence_multi_target()` for multi-word validation. **CAMeL disambiguation**: `lookup_lemma()` calls `_camel_disambiguate()` (via `find_best_db_match()`) for ambiguous clitic-stripped matches and as last-resort fallback. Al-prefix length guard: stems < 3 chars skip al-prepend. **Two-pass lookup**: `build_lemma_lookup()` registers direct bare forms first (pass 1), then forms_json derived forms (pass 2), ensuring bare forms always win over derived forms. Indexes extended forms_json keys (past_3fs, past_3p, imperative, passive_participle). **LLM mapping verification**: `verify_word_mappings_llm()` sends word-lemma pairs to Gemini Flash for contextual homograph disambiguation — active in production (`VERIFY_MAPPINGS_LLM=1`).
- `material_generator.py` — Orchestrates sentence + audio generation for a word. `generate_material_for_word(model_override="gemini")` for on-demand (fast), `warm_sentence_cache(llm_model="gemini")` for in-session background (defaults to Gemini for speed; cron uses `claude_sonnet` via update_material.py). Dynamic difficulty via `get_sentence_difficulty_params()`. Default needed=2, requests needed+2 to absorb validation failures. `store_multi_target_sentence()` for multi-target sentences with correct SentenceWord mappings. `warm_sentence_cache()` **rotates stale sentences first** (via `rotate_stale_sentences()`), then pre-generates using multi-target generation for focus cohort gaps + likely auto-introduction candidates; respects 600-sentence pipeline cap. Also triggered as background task after every session load. **Generate-then-write pattern**: all functions close DB before LLM calls (15-30s via Claude CLI), then reopen briefly for writes — prevents "database is locked" errors during concurrent access. **NULL lemma_id guard**: all sentence storage paths reject unmapped words (uses `build_comprehensive_lemma_lookup()`). **LLM mapping gate**: when `VERIFY_MAPPINGS_LLM=1`, sentences with LLM-flagged bad mappings are discarded (~10% rejection rate).

## Stories & Books
- `story_service.py` — Generate/import stories. Generation uses Claude Opus with self-correction loop: generate once, then iteratively fix unknown words (up to 3 correction rounds) rather than regenerating from scratch. 100% vocabulary compliance required — zero unknown words allowed. `_get_known_words()` includes acquiring words in Leitner box 2+ only (box 1 excluded — too fresh). Full vocab list sent during correction rounds for maximum replacement options. POS-grouped vocab in prompts. Acquiring words highlighted as reinforcement targets. Completion creates "encountered" ULK (no FSRS card); only real FSRS review for words with active cards. Suspend/reactivate toggle via `suspend_story()`. Story statuses: active, completed, suspended. Readiness counts `_ACTIVELY_LEARNING_STATES` (acquiring/learning/known/lapsed) — not just learning/known. `_recalculate_story_counts()` deduplicates by lemma_id (each unique lemma counted once). Function words excluded from unknown_count. `_create_story_words()` checks both surface form and resolved lemma bare form for function word detection. `get_book_page_detail()` for per-page word/sentence breakdown. `_get_book_stats()` computes page-level and story-level progress using `review_log` first-review date to distinguish words genuinely new at import from pre-existing knowledge (resilient to `acquisition_started_at` resets by maintenance scripts).
- `book_import_service.py` — Book import pipeline: per-page OCR → per-page LLM cleanup/diacritics/segmentation → LLM translation → story creation (reuses story_service) → sentence extraction (Sentence + SentenceWord records with source="book", page_number tagged). **LLM mapping verification**: when `VERIFY_MAPPINGS_LLM=1`, runs `verify_word_mappings_llm()` on each book sentence — bad mappings are nulled out (not discarded, since book sentences can't be regenerated). Creates encountered ULK records with source="book" for new words. Cover metadata extraction via Gemini Vision. Book sentences get 1.3x preference in session builder scoring. Words prioritized via story_bonus + page-based bonus (earlier pages → higher priority). CAMeL morphology resolves conjugated forms to existing lemmas. Uploaded images saved to `data/book-uploads/` for retry on failure. Dark image auto-enhancement via Pillow (brightness/contrast boost when mean brightness < 120). Empty OCR results retry with `gemini-2.5-flash-preview` thinking model. Sentences with unmapped tokens kept (lemma_id=None) instead of skipped; StoryWord surface→lemma fallback lookup resolves most unmapped words.

## LLM & NLP
- `llm.py` — Two-tier LLM routing. Background/cron: Claude CLI (free via Max plan) — `claude_sonnet` for sentence gen, `claude_haiku` for quality gate + enrichment + hooks. On-demand: Gemini Flash (fast). `_generate_via_claude_cli()` shells out to `claude -p` with `--output-format json`. MODELS list for API fallback: gemini, openai, anthropic (Haiku), opus. JSON mode, markdown fence stripping, model_override. `format_known_words_by_pos()` for POS-grouped vocabulary. `generate_sentences_multi_target()` for multi-word sentences. `review_sentences_quality()` — Claude Haiku quality gate (grammar + translation accuracy, fail-closed).
- `claude_code.py` — Claude Code CLI (`claude -p`) wrapper. Two modes: (1) `generate_structured()` — no tools, `--json-schema` for single-turn output; (2) `generate_with_tools()` — `--tools "Read,Bash"` + `--dangerously-skip-permissions` + `--add-dir` for multi-turn agentic sessions where Claude reads vocab files and runs validation scripts (timeout: 240s, budget cap: $0.50). `dump_vocabulary_for_claude()` exports full learner vocabulary to prompt file (with "CURRENTLY LEARNING" section for acquiring words) + lookup TSV. Callers fall back to litellm when unavailable.
- `morphology.py` — CAMeL Tools analyzer. Hamza normalized at comparison time only (preserved in storage). Falls back to stub if not installed.
- `transliteration.py` — Deterministic Arabic→ALA-LC romanization from diacritized text. Handles long vowels, shadda, hamza carriers, alif madda/wasla, sun letter assimilation, tāʾ marbūṭa, nisba ending. `transliterate_lemma()` for dictionary form (strips tanwīn + case vowels).
- `variant_detection.py` — Three-layer variant detection: (1) CAMeL candidates with root_id validation (rejects different-root pairs), (2) Gemini Flash LLM confirmation with VariantDecision cache, (3) display fix in sentence_selector uses original lemma_id. Used by ALL import paths. Graceful fallback if LLM unavailable.
- `grammar_service.py` — 24 features, 5 tiers. Comfort score: 60% log-exposure + 40% accuracy, decayed by recency.
- `grammar_tagger.py` — LLM-based grammar feature tagging.
- `grammar_lesson_service.py` — LLM-generated grammar lessons, cached in DB.
- `import_quality.py` — LLM batch classify + filter for word imports. `classify_lemmas()` categorizes each word (standard/proper_name/onomatopoeia/junk) and rejects junk. `filter_useful_lemmas()` is a backward-compat wrapper. Used by OCR, story import, and book import paths.

## Audio & Enrichment
- `tts.py` — ElevenLabs REST, eleven_multilingual_v2, Chaouki voice, speed 0.7. Learner pauses. SHA256 cache.
- `listening.py` — Listening confidence: min(per-word) * 0.6 + avg * 0.4. Requires times_seen ≥ 3, stability ≥ 7d.
- `memory_hooks.py` — LLM-generated memory aids (mnemonic, cognates, collocations, usage context, fun fact). JIT generation on word introduction via background task. Idempotent. Uses Claude Haiku CLI (`model_override="claude_haiku"`, free). Targets 13 learner languages for cognate search. Prompt uses 4-step keyword mnemonic method (Atkinson & Raugh 1975): keyword candidates → pick best → interactive scene → verify meaning extractable. Enforces interactive imagery (keyword + meaning must interact), self-reference ("you"), meaning-as-action, keywords in CAPS. Abstract words get consequence-based or verbal mnemonics. **Premium overgenerate-and-rank**: `regenerate_memory_hooks_premium()` generates 3 candidate mnemonics, self-evaluates each on sound match/interaction/extraction (1-5 scale), picks best. Uses Claude Sonnet. Triggered automatically on: (1) FSRS lapse (fresh transition to "lapsed" state), (2) acquisition box demotion (box 2+ → box 1). Feeds old failed mnemonic as negative example. Always overwrites existing hooks.
- `lemma_enrichment.py` — Batch enrichment for newly created Lemma records. `enrich_lemmas_batch(lemma_ids)` populates forms_json, etymology_json, memory_hooks_json, transliteration_ala_lc. Uses Claude Haiku CLI (`claude_haiku`) for forms and etymology (free). Runs as background task after book/story import. Opens own DB session. Cron catch-all in update_material.py Step E enriches all unenriched lemmas per run.
- `soniox_service.py` — Soniox Speech-to-Text REST API wrapper. Async file transcription with Arabic+English code-switching, speaker diarization, word-level timestamps. Used by `import_michel_thomas.py`. Key: SONIOX_API_KEY in `.env`.

## Other
- `ocr_service.py` — Gemini Vision OCR: text extraction, word extraction (OCR→morphology→LLM translation), textbook page processing. Detects printed page numbers from textbook images (`textbook_page_number` on PageUpload). `_call_gemini_vision()` supports `model_override` param. Timeout: 300s. `start_acquiring` toggle: when true, words start acquisition immediately (box 1, due_immediately); when false, creates "encountered" ULK. Runs variant detection after import (resets variant ULKs from acquiring→encountered).
- `interaction_logger.py` — Append-only JSONL. Skipped when TESTING env var set.
- `flag_evaluator.py` — Background LLM evaluation of flagged content. Handles: word_gloss (GPT-5.2), sentence_arabic/english/transliteration (GPT-5.2), word_mapping (Claude CLI haiku — re-evaluates word-lemma mappings, auto-fixes if correct lemma exists in DB). Auto-fixes or retires. Writes to ActivityLog.
- `activity_log.py` — Shared helper for writing ActivityLog entries.
